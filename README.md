 # **ğŸ¸ Swiftie-GPT: From-Scratch Transformer**

Bu proje, Taylor Swift'in lirik dÃ¼nyasÄ±nÄ± modellemek iÃ§in PyTorch kullanÄ±larak sÄ±fÄ±rdan inÅŸa edilmiÅŸ bir Decoder-only Transformer mimarisidir. HazÄ±r model (GPT, Llama vb.) kullanmak yerine; tokenizer'dan dikkat mekanizmasÄ±na kadar her bileÅŸen manuel olarak kodlanmÄ±ÅŸtÄ±r.


## **ğŸ§  Teknik Mimari**

### **1. Hybrid Tokenizer & Dataset**

* **Greedy Tokenization:** Regex tabanlÄ± ([a-zA-Z']+|[0-9]+|[?!,.]|\s+) hibrit bir yapÄ± kullanÄ±ldÄ±. Bilinen kelimeleri tam, bilinmeyenleri karakter bazlÄ± iÅŸleyerek esneklik saÄŸlar.

* **Sliding Window:** Veri seti, her adÄ±mda bir token kaydÄ±rarak modeli bir sonraki karakteri tahmin etmeye zorlayan (x, y) Ã§iftleri Ã¼retir.

### **2. Sinusoidal Embedding**

* **Positional Encoding:** Kelime sÄ±rasÄ±nÄ± anlamak iÃ§in eÄŸitilebilir embedding yerine sabit sinÃ¼s/kosinÃ¼s dalgalarÄ± kullanÄ±lmÄ±ÅŸtÄ±r. Bu, modelin uzun dizilerdeki zamansal iliÅŸkiyi matematiksel bir hassasiyetle kavramasÄ±nÄ± saÄŸlar.

### **3. Causal Multi-Head Attention**

* **Self-Attention:** Kelimeler arasÄ± anlamsal baÄŸlarÄ± Q,K,V matrisleri Ã¼zerinden Ã§Ã¶zer.

* **Masking:** torch.tril ile modelin eÄŸitim sÄ±rasÄ±nda "geleceÄŸi gÃ¶rmesi" engellenmiÅŸtir.

* **Multi-Head:** 4 paralel kafa ile metnin farklÄ± anlamsal boyutlarÄ±na (kafiye, Ã¶zne-yÃ¼klem vb.) aynÄ± anda odaklanÄ±r.

### **4. Gated Decoder & MLP**

* **Gated Projection:** Klasik MLP yerine Llama 3 tarzÄ± "Gated Linear Unit" ve GeLU aktivasyonu kullanÄ±lmÄ±ÅŸtÄ±r. Bu "kapÄ±" mekanizmasÄ± modelin Ã¶ÄŸrenme kapasitesini artÄ±rÄ±r.

* **Manual LayerNorm:** Stabil bir eÄŸitim iÃ§in normalizasyon katmanÄ± sÄ±fÄ±rdan matematiksel formÃ¼lÃ¼yle kodlanmÄ±ÅŸtÄ±r.

### **5. Generation**

* **Top-K & Temperature:** Ãœretim sÄ±rasÄ±nda "yaratÄ±cÄ±lÄ±k" ayarÄ± yapÄ±lÄ±r. Top-K ile saÃ§ma ihtimaller elenirken, Temperature ile modelin risk alma seviyesi (yaratÄ±cÄ±lÄ±ÄŸÄ±) belirlenir.

## **âš™ï¸ EÄŸitim ve Hiper-Parametreler**

### **Parametre DeÄŸer AÃ§Ä±klama**

* **Context Length** 128 Modelin bir seferde baktÄ±ÄŸÄ± karakter penceresi

* **Batch Size** 16 Her adÄ±mda iÅŸlenen Ã¶rnek sayÄ±sÄ±

* **Embedding Dim** 128 Kelimelerin temsil edildiÄŸi vektÃ¶r boyutu

* **Num Heads** 4 Multi-head attention kafa sayÄ±sÄ±

* **Num Layers** 6 Ãœst Ã¼ste binen Decoder bloÄŸu sayÄ±sÄ±

* **Learning Rate** 5e-4 AdamW optimizer Ã¶ÄŸrenme oranÄ±

* **Epochs** 100 Toplam eÄŸitim tur sayÄ±sÄ±

  * **Checkpointing:** EÄŸitim sonunda model, sÃ¶zlÃ¼k ve konfigÃ¼rasyonla birlikte model.pth olarak kaydedilir.

  * **Visuals:** EÄŸitim sÃ¼reci izlenerek hata payÄ±nÄ± gÃ¶steren loss_curve.png grafiÄŸi Ã¼retilir.

## **ğŸš€ KullanÄ±cÄ± ArayÃ¼zÃ¼**

Modeli test etmek iÃ§in Gradio tabanlÄ± modern bir web arayÃ¼zÃ¼ sunulmuÅŸtur:

* **Temperature:** Modelin risk alma/yaratÄ±cÄ±lÄ±k seviyesini ayarlar.

* **Top-K:** En yÃ¼ksek olasÄ±lÄ±klÄ± k kelime arasÄ±ndan seÃ§im yaparak tutarlÄ±lÄ±ÄŸÄ± korur. 
